\chapter{Background}\label{chap:bg}

\section{Computer aid cooking activities}

Cooking and eating have been the most fundamental activities of humankind since the days of trading caravans in the ancient days until now, which affect various aspects of human life such as health, dietary, human communication, safety of food, entertainment, culinary art, welfare, and so on. However, many people who cook at home require supports for cooking because it requires experience and knowledge. They may also need support for food-logging and menu planning for the health of their family. Needless to say, support for a good and enjoyable dinner would improve the quality of life. On the other hand, systematic cooking and eating support for the elderly and/or physically challenged people are significantly important.

Together with the development of technology and the availability of equipment in cooking, many supporting systems are introduced. For example, Morioka et al.~\cite{morioka:camera-projecter} proposes a new cooking support system that cameras and projectors are installed above the kitchen counter. Information such as size, position and direction of the ingredient are measured by analyzing the camera image. The projector displays necessary information onto the ingredient in the overlay by using the information.

In another branch, Nakauchi, Y et al.~\cite{nakauchi:recog} propose a cooking support system by using ubiquitous sensors. They developed machine learning algorithm that recognizes cooking procedures by taking account of various and numerous sensor information and past human behaviors. To confirme the feasibility of the algorithm they proposed a cooking support system that automatically displays cooking instruction movies according to users progresses. 

Smartphone nowadays is integrating many sensors and camera and it becomes popular among users. Villalobos, G et al.~\cite{villalobos:image-calorie} propose a smart system that takes advantage of the technologies available for the smartphones, to build an application to measure and monitor the daily calorie intake for obese and overweight patients. The system records a photo of the food before and after eating in order to estimate the consumption calorie of the selected food and its nutrient components.

Systems with sensors or camera are not the only way to support cooking and eating activities. Ide, I et al.~\cite{ide:inexper} proposed a method that detects difficult descriptions for an inexperienced user in an existing text recipe, and supplements them with multimedia contents including text information extracted from a large number of recipes, and also images and video clips on certain kinds of cooking operations, to facilitate the understanding of the recipe the system which helps inexperienced users in understanding non-professional recipe descriptions.

There are also other ideas and approaches to develop a cooking and eating activities supporting system that we can not list up in this thesis, but there is a fact that cooking and eating activities supporting research has became a new trend in recent years.

\section{The TF-IDF Numerical Statistic}

In the existing cooking support systems, the methods vary such as image processing, text retrieval, sensing, etc. We use the text processing approach to directly analyze the recipes with their ingredients and associated amount. In this research we use a method named $TF-IDF$, which is robust and originally used for weighting word and documents. 

In this chapter, we introduce background of $TF-IDF$ method, the idea, applications, mathematical definition and problems in section 2.1, 2.2, 2.3 respectively.  

\clearpage
\subsection{Overview of TF-IDF Algorithm and Its Application}\label{sec:bg_intro}


One of the earliest and most popular ways to create weighting vectors is the $TF-IDF$ family of weighting schemes.  

In 1972, Karen Sp̈arck Jones published in the Journal of Documentation a paper called ``A statistical interpretation of term specificity and its application in retrieval''~\cite{Jones72astatistical}. The measure of term specificity first proposed in that paper later became known as inverse document frequency, or $IDF$; it is based on counting the number of documents in the collection being searched which contain (or are indexed by) the term in question. The intuition was that a query term which occurs in many documents is not a good discriminator, and should be given less weight than one which occurs in few documents, and the measure was an heuristic implementation of this intuition.
The intuition, and the measure associated with it, proved to be a giant leap in the field of information retrieval. Coupled with $TF$ (the frequency of the term in the document itself, in this case, the more the better), it found its way into almost every term weighting scheme.
The class of weighting schemes known generically as $TF \times IDF$, which involve multiplying the $IDF$ measure (possibly one of a number of variants) by a $TF$ measure (again possibly one of a number of variants, not just the raw count) have proved extraordinarily robust and difficult to beat, even by much more carefully worked out models and theories. It has even made its way outside of text retrieval into methods for retrieval of other media, and into language processing techniques for other purposes.

For example, say that we have a set of English text documents and wish to determine which document is most relevant to the query "a good man". A simple way to start out is by eliminating documents that do not contain all three words "a", "good", and "man", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its term frequency.

However, because the term "a" is so common, this will tend to incorrectly emphasize documents which happen to use the word "a" more frequently, without giving enough weight to the more meaningful terms "good" and "man". The term "a" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words "good" and "man". Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.


Variations of the $TF-IDF$ weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. $TF-IDF$ can be successfully used for stop-words filtering in various subject fields including text summarization and classification.


\subsection{Mathematical Details}\label{sec:bg_detail}

The inverse document frequency is a measure of whether the term is common or rare across all documents. It is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.

\begin{center}
\smallskip
$\mathrm{idf}(t, D) = \log \frac{\displaystyle |D|}{\displaystyle |\{d \in D: t \in d\}|}$
\smallskip
\end{center}


where $|D|$ is cardinality of D, or the total number of documents in the corpus and $|\{d \in D: t \in d\}|$ is number of documents where the term t appears (i.e., $\mathrm{tf}(t,d) \neq 0)$. \\

If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\{d \in D: t \in d\}|$.

Mathematically the base of the log function does not matter and constitutes a constant multiplicative factor towards the overall result.

Then $TF–IDF$ is calculated as
\begin{center}
\smallskip
$\mathrm{tfidf}(t,d,D) = \mathrm{tf}(t,d) \times \mathrm{idf}(t, D)$ 
\smallskip
\end{center}

The formula above is the simplest way to implement $TF-IDF$ weighting scheme. Different schemes are used in specific case depends on the problems the system solves. Our research also use the basic idea of $TF-IDF$ scheme but designed for specific problems of analyzing regional food's taste.  


\subsection{Related Problems to TF-IDF}\label{sec:bg_prob}

Though $TF-IDF$ is a robust weighting scheme, for different systems it is adapted in different ways and depends on the problems. 

For the system in which the database of documents is often updated, typically new documents are received over time. In this case, the $TF$ value of old documents are fixed, there is no need to recalculate but the $IDF$ value certainly changed. Recalculation is necessary but choosing which mechanics varies according to systems.

One option is to keep using the existing $TF-IDF$ until a certain number of new documents have been received, and then recalculate. But there are systems that require update instantly, these system will encounter the problem of massive calculation. Especially in our regional food's featured ingredients recognizing system, because of the way we apply the $TF-IDF$ algorithm, the recalculation on updating data became more expensive. We will propose a method to solve this problems in chapter 4.     

